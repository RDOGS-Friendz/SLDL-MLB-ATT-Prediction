{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import _pickle as cPickle\n",
    "from os.path import exists\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def GetMetaGameInfo(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    scorebox = soup.find(\"div\", {\"class\": \"scorebox\"})\n",
    "    t1, t2, score_box_meta = (list(scorebox.children)[i] for i in [1,3,5])\n",
    "    for t in [t1, t2]:\n",
    "        t.name = t.find(\"strong\").text.strip(\" \").strip(\"\\n\").replace(\" \", \"\").replace(\".\", \"\")\n",
    "        t.score = int(t.find(\"div\", {\"class\": \"score\"}).text)\n",
    "        t.pre_win, t.pre_loss = (int(i) for i in list(t.children)[4].text.split('-'))\n",
    "    if t1.score > t2.score:\n",
    "        t1.pre_win -= 1\n",
    "        t2.pre_loss -= 1\n",
    "    else:\n",
    "        t2.pre_win -= 1\n",
    "        t1.pre_loss -= 1\n",
    "    scorebox = soup.find(\"div\", {\"class\": \"scorebox\"})\n",
    "    t1, t2, score_box_meta = (list(scorebox.children)[i] for i in [1,3,5])\n",
    "    rows = [i.text for i in list(score_box_meta.children) if i != '\\n']\n",
    "    date = rows[0]\n",
    "    start_time = [row for row in rows if \"Start Time:\" in row][0]\n",
    "    att = [row for row in rows if \"Attendance:\" in row]\n",
    "    have_att = len(att) != 0\n",
    "    att = -1 if have_att == False else att[0]\n",
    "    venue = [row for row in rows if \"Venue:\" in row][0]\n",
    "    duration = [row for row in rows if \"Game Duration:\" in row][0]\n",
    "    at_night_on_grass = [row for row in rows if \", on \" in row][0]\n",
    "    # date, start_time, att, venue, duration, at_night_on_grass = list(list(score_box_meta.children)[i].text for i in [1, 2, 3, 4, 5, 6] )\n",
    "    at_night = None\n",
    "    on_grass = None\n",
    "    info_dict = dict()\n",
    "    if have_att:\n",
    "        date = datetime.strptime(date, \"%A, %B %d, %Y\")\n",
    "        start_time = datetime.strptime(\"\".join(start_time.split(\" \")[2:4]).replace(\".\", \"\"), \"%I:%M%p\")\n",
    "        start_time = date + timedelta(hours=start_time.hour, minutes=start_time.minute)\n",
    "        duration = datetime.strptime(duration[15:], \"%H:%M\")\n",
    "        duration = timedelta(hours=duration.hour, minutes=duration.minute)\n",
    "        at_night, on_grass = at_night_on_grass.split(\", \")\n",
    "        at_night = 'Night Game' == at_night\n",
    "        on_grass = 'on grass' == on_grass\n",
    "        att = int(att.split(\": \")[1].replace(\",\", \"\"))\n",
    "        venue = venue.split(\": \")[1]\n",
    "        infos = list(soup.find('h2', text=\"Other Info\").parent.parent.find(\"div\", {\"class\" : \"section_content\"}).children)\n",
    "        infos = [i for i in infos if '\\n' != i]\n",
    "        for info in infos:\n",
    "            k,v = info.text.split(\":\", 1)\n",
    "            info_dict[k] = v\n",
    "    gameInfo = {\n",
    "        \"have_att\": have_att,\n",
    "        \"start_time\": start_time, \"duration\": duration,\n",
    "        \"venue\": venue, \"at_night\": at_night, \"on_grass\": on_grass,\n",
    "        'Start Time Weather': \"\" if 'Start Time Weather' not in info_dict else  info_dict['Start Time Weather'],\n",
    "        'Umpires': \"\" if \"Umpires\" not in info_dict else info_dict['Umpires'],\n",
    "        \"att\": att\n",
    "    }\n",
    "    return t1, t2, gameInfo\n",
    "\n",
    "# soup = BeautifulSoup(, \"html.parser\")\n",
    "def GetTable(html, table_id):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    tbody = soup.find(\"div\", {\"id\": table_id}).find(\"tbody\")\n",
    "    table = []\n",
    "    for tr in tbody.findAll(\"tr\"):\n",
    "        row = []\n",
    "        name = tr.find(\"th\").text\n",
    "        row.append(name)\n",
    "        for td in tr.findAll(\"td\"):\n",
    "            row.append(td.text)\n",
    "        if not all([i == \"\" for i in row]):\n",
    "            table.append(row)\n",
    "    return pd.DataFrame(table, columns=[\n",
    "        'Batting',\n",
    "        'AB',\n",
    "        'R',\n",
    "        'H',\n",
    "        'RBI',\n",
    "        'BB',\n",
    "        'SO',\n",
    "        'PA',\n",
    "        'BA',\n",
    "        'OBP',\n",
    "        'SLG',\n",
    "        'OPS',\n",
    "        'Pit',\n",
    "        'Str',\n",
    "        'WPA',\n",
    "        'aLI',\n",
    "        'WPA+',\n",
    "        'WPA-',\n",
    "        'cWPA',\n",
    "        'acLI',\n",
    "        'RE24',\n",
    "        'PO',\n",
    "        'A',\n",
    "        'Details'\n",
    "    ])\n",
    "    \n",
    "def setup_driver():\n",
    "    options = Options()\n",
    "    options.headless = False\n",
    "    \n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "\n",
    "    ### This blocks images and javascript requests\n",
    "    chrome_prefs = {\n",
    "        \"profile.default_content_setting_values\": {\n",
    "            \"images\": 2,\n",
    "        }\n",
    "    }\n",
    "    chrome_options.experimental_options[\"prefs\"] = chrome_prefs\n",
    "\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager(\n",
    "    ).install()), options=options)#, chrome_options=chrome_options)\n",
    "    driver.set_page_load_timeout(4)\n",
    "    return driver\n",
    "\n",
    "def scratch_meta_page(url):\n",
    "    driver = setup_driver()\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            driver.get(url)\n",
    "        except TimeoutException:\n",
    "            print(\"load page timeout\")\n",
    "\n",
    "        try:\n",
    "            WebDriverWait(driver, 1).until(EC.presence_of_element_located(\n",
    "                    (By.CLASS_NAME, \"game\")))\n",
    "        except TimeoutException:\n",
    "            continue\n",
    "        break\n",
    "    print(\"get meta page data\")\n",
    "    return driver.page_source\n",
    "\n",
    "\n",
    "def scratch_single_page(url):\n",
    "    print(f\"Scratching {url}\")\n",
    "    driver=setup_driver()\n",
    "\n",
    "    # load page for 4 sec\n",
    "    try:\n",
    "        driver.get(url)\n",
    "    except TimeoutException:\n",
    "        print(\"load page timeout\")\n",
    "\n",
    "    team1, team2, metaGameInfo = GetMetaGameInfo(driver.page_source)\n",
    "    df1=GetTable(driver.page_source, f\"all_{team1.name}batting\")\n",
    "    df2=GetTable(driver.page_source, f\"all_{team2.name}batting\")\n",
    "    print(\"get page data success\")\n",
    "    \n",
    "    team1={\n",
    "        \"name\": team1.name,\n",
    "        \"pre_win\": team1.pre_win,\n",
    "        \"pre_loss\": team1.pre_loss,\n",
    "        \"player_df\": df1\n",
    "    }\n",
    "    team2={\n",
    "        \"name\": team2.name,\n",
    "        \"pre_win\": team2.pre_win,\n",
    "        \"pre_loss\": team2.pre_loss,\n",
    "        \"player_df\": df2\n",
    "    }\n",
    "\n",
    "    gameInfo={\n",
    "        \"meta_game_info\": metaGameInfo,\n",
    "        \"team1\": team1,\n",
    "        \"team2\": team2\n",
    "    }\n",
    "\n",
    "    return gameInfo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_by_years(years):\n",
    "    \n",
    "    for year in years:\n",
    "        print(f\"get games from year: {year}\")\n",
    "        # load previous scratched games data\n",
    "        if exists(f\"gamesData{year}.pickle\"):\n",
    "            with open(f\"gamesData{year}.pickle\", \"rb\") as output_file:\n",
    "                data = cPickle.load(output_file)\n",
    "        else:\n",
    "            data = dict()\n",
    "        \n",
    "        # get all year's game from schedule page\n",
    "        html = scratch_meta_page(f\"https://www.baseball-reference.com/leagues/majors/{year}-schedule.shtml\")\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        games = soup.findAll(\"p\", {\"class\": \"game\"})\n",
    "        print(f\"have {len(games)} to scrap\")\n",
    "        # scratch the rest\n",
    "        for game in games:\n",
    "            for i in range(5):# max retry 5 time\n",
    "                game_url = \"https://www.baseball-reference.com\" + game.find(\"em\").find(\"a\")['href']\n",
    "                if game_url not in data:\n",
    "                    #scratch game\n",
    "                    try:\n",
    "                        gameInfo = scratch_single_page(game_url)\n",
    "                        # save to data\n",
    "                        data[game_url] = gameInfo\n",
    "                        with open(f\"gamesData{year}.pickle\", \"wb\") as output_file:\n",
    "                            cPickle.dump(data, output_file)\n",
    "                        break\n",
    "                    except AttributeError:\n",
    "                        print(\"something wrong, retry scrap this page\")\n",
    "                        continue\n",
    "                    except ValueError:\n",
    "                        print(\"something wrong, retry scrap this page\")\n",
    "                        continue\n",
    "                else:\n",
    "                    break\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# year to scrap\n",
    "years = [\"2022\", \"2021\", \"2019\", \"2018\", \"2017\", \"2016\", \"2015\"]\n",
    "scrap_by_years(years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2470\n",
      "2466\n",
      "2466\n",
      "2464\n",
      "2468\n",
      "2463\n",
      "2464\n"
     ]
    }
   ],
   "source": [
    "for y in years:\n",
    "    with open(f\"gamesData{y}.pickle\", \"rb\") as output_file:\n",
    "                data = cPickle.load(output_file)\n",
    "    print(len(data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "43df744c98c246816ca7c9cbc62e6caeb27475d47e1cd721e7fc30f17cf8524c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
