{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import _pickle as cPickle\n",
    "from os.path import exists\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def GetMetaGameInfo(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    scorebox = soup.find(\"div\", {\"class\": \"scorebox\"})\n",
    "    t1, t2, score_box_meta = (list(scorebox.children)[i] for i in [1,3,5])\n",
    "    for t in [t1, t2]:\n",
    "        t.name = t.find(\"strong\").text.strip(\" \").strip(\"\\n\").replace(\" \", \"\").replace(\".\", \"\")\n",
    "        t.score = int(t.find(\"div\", {\"class\": \"score\"}).text)\n",
    "        t.pre_win, t.pre_loss = (int(i) for i in list(t.children)[4].text.split('-'))\n",
    "    if t1.score > t2.score:\n",
    "        t1.pre_win -= 1\n",
    "        t2.pre_loss -= 1\n",
    "    else:\n",
    "        t2.pre_win -= 1\n",
    "        t1.pre_loss -= 1\n",
    "    scorebox = soup.find(\"div\", {\"class\": \"scorebox\"})\n",
    "    t1, t2, score_box_meta = (list(scorebox.children)[i] for i in [1,3,5])\n",
    "    rows = [i.text for i in list(score_box_meta.children) if i != '\\n']\n",
    "    date = rows[0]\n",
    "    start_time = [row for row in rows if \"Start Time:\" in row][0]\n",
    "    att = [row for row in rows if \"Attendance:\" in row]\n",
    "    have_att = len(att) != 0\n",
    "    att = -1 if have_att == False else att[0]\n",
    "    venue = [row for row in rows if \"Venue:\" in row][0]\n",
    "    duration = [row for row in rows if \"Game Duration:\" in row][0]\n",
    "    at_night_on_grass = [row for row in rows if \", on \" in row][0]\n",
    "    # date, start_time, att, venue, duration, at_night_on_grass = list(list(score_box_meta.children)[i].text for i in [1, 2, 3, 4, 5, 6] )\n",
    "    at_night = None\n",
    "    on_grass = None\n",
    "    info_dict = dict()\n",
    "    if have_att:\n",
    "        date = datetime.strptime(date, \"%A, %B %d, %Y\")\n",
    "        start_time = datetime.strptime(\"\".join(start_time.split(\" \")[2:4]).replace(\".\", \"\"), \"%I:%M%p\")\n",
    "        start_time = date + timedelta(hours=start_time.hour, minutes=start_time.minute)\n",
    "        duration = datetime.strptime(duration[15:], \"%H:%M\")\n",
    "        duration = timedelta(hours=duration.hour, minutes=duration.minute)\n",
    "        at_night, on_grass = at_night_on_grass.split(\", \")\n",
    "        at_night = 'Night Game' == at_night\n",
    "        on_grass = 'on grass' == on_grass\n",
    "        att = int(att.split(\": \")[1].replace(\",\", \"\"))\n",
    "        venue = venue.split(\": \")[1]\n",
    "        infos = list(soup.find('h2', text=\"Other Info\").parent.parent.find(\"div\", {\"class\" : \"section_content\"}).children)\n",
    "        infos = [i for i in infos if '\\n' != i]\n",
    "        for info in infos:\n",
    "            k,v = info.text.split(\":\", 1)\n",
    "            info_dict[k] = v\n",
    "    gameInfo = {\n",
    "        \"have_att\": have_att,\n",
    "        \"start_time\": start_time, \"duration\": duration,\n",
    "        \"venue\": venue, \"at_night\": at_night, \"on_grass\": on_grass,\n",
    "        'Start Time Weather': \"\" if 'Start Time Weather' not in info_dict else  info_dict['Start Time Weather'],\n",
    "        'Umpires': \"\" if \"Umpires\" not in info_dict else info_dict['Umpires'],\n",
    "        \"att\": att\n",
    "    }\n",
    "    return t1, t2, gameInfo\n",
    "\n",
    "# soup = BeautifulSoup(, \"html.parser\")\n",
    "def GetTable(html, table_id):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    tbody = soup.find(\"div\", {\"id\": table_id}).find(\"tbody\")\n",
    "    table = []\n",
    "    for tr in tbody.findAll(\"tr\"):\n",
    "        row = []\n",
    "        name = tr.find(\"th\").text\n",
    "        row.append(name)\n",
    "        for td in tr.findAll(\"td\"):\n",
    "            row.append(td.text)\n",
    "        if not all([i == \"\" for i in row]):\n",
    "            table.append(row)\n",
    "    return pd.DataFrame(table, columns=[\n",
    "        'Batting',\n",
    "        'AB',\n",
    "        'R',\n",
    "        'H',\n",
    "        'RBI',\n",
    "        'BB',\n",
    "        'SO',\n",
    "        'PA',\n",
    "        'BA',\n",
    "        'OBP',\n",
    "        'SLG',\n",
    "        'OPS',\n",
    "        'Pit',\n",
    "        'Str',\n",
    "        'WPA',\n",
    "        'aLI',\n",
    "        'WPA+',\n",
    "        'WPA-',\n",
    "        'cWPA',\n",
    "        'acLI',\n",
    "        'RE24',\n",
    "        'PO',\n",
    "        'A',\n",
    "        'Details'\n",
    "    ])\n",
    "    \n",
    "def setup_driver():\n",
    "    options = Options()\n",
    "    options.headless = False\n",
    "    \n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "\n",
    "    ### This blocks images and javascript requests\n",
    "    chrome_prefs = {\n",
    "        \"profile.default_content_setting_values\": {\n",
    "            \"images\": 2,\n",
    "        }\n",
    "    }\n",
    "    chrome_options.experimental_options[\"prefs\"] = chrome_prefs\n",
    "\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager(\n",
    "    ).install()), options=options)#, chrome_options=chrome_options)\n",
    "    driver.set_page_load_timeout(4)\n",
    "    return driver\n",
    "\n",
    "def scratch_meta_page(url):\n",
    "    driver = setup_driver()\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            driver.get(url)\n",
    "        except TimeoutException:\n",
    "            print(\"load page timeout\")\n",
    "\n",
    "        try:\n",
    "            WebDriverWait(driver, 1).until(EC.presence_of_element_located(\n",
    "                    (By.CLASS_NAME, \"game\")))\n",
    "        except TimeoutException:\n",
    "            continue\n",
    "        break\n",
    "    print(\"get meta page data\")\n",
    "    return driver.page_source\n",
    "\n",
    "\n",
    "def scratch_single_page(url):\n",
    "    print(f\"Scratching {url}\")\n",
    "    driver=setup_driver()\n",
    "\n",
    "    # load page for 4 sec\n",
    "    try:\n",
    "        driver.get(url)\n",
    "    except TimeoutException:\n",
    "        print(\"load page timeout\")\n",
    "\n",
    "    team1, team2, metaGameInfo = GetMetaGameInfo(driver.page_source)\n",
    "    df1=GetTable(driver.page_source, f\"all_{team1.name}batting\")\n",
    "    df2=GetTable(driver.page_source, f\"all_{team2.name}batting\")\n",
    "    print(\"get page data success\")\n",
    "    \n",
    "    team1={\n",
    "        \"name\": team1.name,\n",
    "        \"pre_win\": team1.pre_win,\n",
    "        \"pre_loss\": team1.pre_loss,\n",
    "        \"player_df\": df1\n",
    "    }\n",
    "    team2={\n",
    "        \"name\": team2.name,\n",
    "        \"pre_win\": team2.pre_win,\n",
    "        \"pre_loss\": team2.pre_loss,\n",
    "        \"player_df\": df2\n",
    "    }\n",
    "\n",
    "    gameInfo={\n",
    "        \"meta_game_info\": metaGameInfo,\n",
    "        \"team1\": team1,\n",
    "        \"team2\": team2\n",
    "    }\n",
    "\n",
    "    return gameInfo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_by_years(years):\n",
    "    for year in years:\n",
    "        print(f\"get games from year: {year}\")\n",
    "        # load previous scratched games data\n",
    "        if exists(f\"gamesData{year}.pickle\"):\n",
    "            with open(f\"gamesData{year}.pickle\", \"rb\") as output_file:\n",
    "                data = cPickle.load(output_file)\n",
    "        else:\n",
    "            data = dict()\n",
    "        \n",
    "        # get all year's game from schedule page\n",
    "        html = scratch_meta_page(f\"https://www.baseball-reference.com/leagues/majors/{year}-schedule.shtml\")\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        games = soup.findAll(\"p\", {\"class\": \"game\"})\n",
    "        print(f\"have {len(games)} to scrap\")\n",
    "        # scratch the rest\n",
    "        for game in games:\n",
    "            for i in range(5):# max retry 5 time\n",
    "                game_url = \"https://www.baseball-reference.com\" + game.find(\"em\").find(\"a\")['href']\n",
    "                if game_url not in data:\n",
    "                    #scratch game\n",
    "                    # try:\n",
    "                        gameInfo = scratch_single_page(game_url)\n",
    "                        # save to data\n",
    "                        data[game_url] = gameInfo\n",
    "                        with open(f\"gamesData{year}.pickle\", \"wb\") as output_file:\n",
    "                            cPickle.dump(data, output_file)\n",
    "                        break\n",
    "                    # except AttributeError:\n",
    "                    #     print(\"something wrong, retry scrap this page\")\n",
    "                    #     continue\n",
    "                    # except ValueError:\n",
    "                    #     print(\"something wrong, retry scrap this page\")\n",
    "                        # continue\n",
    "                else:\n",
    "                    break\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scratching https://www.baseball-reference.com/boxes/BOS/BOS201906290.shtml\n",
      "load page timeout\n",
      "Saturday, June 29, 2019 Start Time: 6:10 p.m. Local Game Duration: 4:42 Venue: London Stadium\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m html \u001b[39m=\u001b[39m scratch_single_page(\u001b[39m\"\u001b[39;49m\u001b[39mhttps://www.baseball-reference.com/boxes/BOS/BOS201906290.shtml\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      2\u001b[0m soup \u001b[39m=\u001b[39m BeautifulSoup(html, \u001b[39m\"\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn [12], line 145\u001b[0m, in \u001b[0;36mscratch_single_page\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[39mexcept\u001b[39;00m TimeoutException:\n\u001b[0;32m    143\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mload page timeout\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 145\u001b[0m team1, team2, metaGameInfo \u001b[39m=\u001b[39m GetMetaGameInfo(driver\u001b[39m.\u001b[39;49mpage_source)\n\u001b[0;32m    146\u001b[0m df1\u001b[39m=\u001b[39mGetTable(driver\u001b[39m.\u001b[39mpage_source, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mall_\u001b[39m\u001b[39m{\u001b[39;00mteam1\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39mbatting\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    147\u001b[0m df2\u001b[39m=\u001b[39mGetTable(driver\u001b[39m.\u001b[39mpage_source, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mall_\u001b[39m\u001b[39m{\u001b[39;00mteam2\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39mbatting\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn [12], line 37\u001b[0m, in \u001b[0;36mGetMetaGameInfo\u001b[1;34m(html)\u001b[0m\n\u001b[0;32m     35\u001b[0m duration \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mstrptime(duration[\u001b[39m15\u001b[39m:], \u001b[39m\"\u001b[39m\u001b[39m%\u001b[39m\u001b[39mH:\u001b[39m\u001b[39m%\u001b[39m\u001b[39mM\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     36\u001b[0m duration \u001b[39m=\u001b[39m timedelta(hours\u001b[39m=\u001b[39mduration\u001b[39m.\u001b[39mhour, minutes\u001b[39m=\u001b[39mduration\u001b[39m.\u001b[39mminute)\n\u001b[1;32m---> 37\u001b[0m at_night, on_grass \u001b[39m=\u001b[39m at_night_on_grass\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     38\u001b[0m at_night \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mNight Game\u001b[39m\u001b[39m'\u001b[39m \u001b[39m==\u001b[39m at_night\n\u001b[0;32m     39\u001b[0m on_grass \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mon grass\u001b[39m\u001b[39m'\u001b[39m \u001b[39m==\u001b[39m on_grass\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "# html = scratch_single_page(\"https://www.baseball-reference.com/boxes/BOS/BOS201906290.shtml\")\n",
    "# soup = BeautifulSoup(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get games from year: 2022\n",
      "load page timeout\n",
      "get meta page data\n",
      "have 2470 to scrap\n",
      "get games from year: 2021\n",
      "load page timeout\n",
      "get meta page data\n",
      "have 2466 to scrap\n",
      "get games from year: 2019\n",
      "load page timeout\n",
      "get meta page data\n",
      "have 2466 to scrap\n",
      "Scratching https://www.baseball-reference.com/boxes/BOS/BOS201906290.shtml\n",
      "load page timeout\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [5], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m y2 \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39m2019\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m2018\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m      5\u001b[0m y3 \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39m2017\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m2016\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m2015\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m----> 7\u001b[0m scrap_by_years(years)\n",
      "Cell \u001b[1;32mIn [3], line 23\u001b[0m, in \u001b[0;36mscrap_by_years\u001b[1;34m(years)\u001b[0m\n\u001b[0;32m     19\u001b[0m game_url \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps://www.baseball-reference.com\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m game\u001b[39m.\u001b[39mfind(\u001b[39m\"\u001b[39m\u001b[39mem\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mfind(\u001b[39m\"\u001b[39m\u001b[39ma\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m'\u001b[39m\u001b[39mhref\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     20\u001b[0m \u001b[39mif\u001b[39;00m game_url \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m data:\n\u001b[0;32m     21\u001b[0m     \u001b[39m#scratch game\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     \u001b[39m# try:\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m         gameInfo \u001b[39m=\u001b[39m scratch_single_page(game_url)\n\u001b[0;32m     24\u001b[0m         \u001b[39m# save to data\u001b[39;00m\n\u001b[0;32m     25\u001b[0m         data[game_url] \u001b[39m=\u001b[39m gameInfo\n",
      "Cell \u001b[1;32mIn [2], line 144\u001b[0m, in \u001b[0;36mscratch_single_page\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[39mexcept\u001b[39;00m TimeoutException:\n\u001b[0;32m    142\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mload page timeout\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 144\u001b[0m team1, team2, metaGameInfo \u001b[39m=\u001b[39m GetMetaGameInfo(driver\u001b[39m.\u001b[39;49mpage_source)\n\u001b[0;32m    145\u001b[0m df1\u001b[39m=\u001b[39mGetTable(driver\u001b[39m.\u001b[39mpage_source, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mall_\u001b[39m\u001b[39m{\u001b[39;00mteam1\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39mbatting\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    146\u001b[0m df2\u001b[39m=\u001b[39mGetTable(driver\u001b[39m.\u001b[39mpage_source, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mall_\u001b[39m\u001b[39m{\u001b[39;00mteam2\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39mbatting\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn [2], line 36\u001b[0m, in \u001b[0;36mGetMetaGameInfo\u001b[1;34m(html)\u001b[0m\n\u001b[0;32m     34\u001b[0m duration \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mstrptime(duration[\u001b[39m15\u001b[39m:], \u001b[39m\"\u001b[39m\u001b[39m%\u001b[39m\u001b[39mH:\u001b[39m\u001b[39m%\u001b[39m\u001b[39mM\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     35\u001b[0m duration \u001b[39m=\u001b[39m timedelta(hours\u001b[39m=\u001b[39mduration\u001b[39m.\u001b[39mhour, minutes\u001b[39m=\u001b[39mduration\u001b[39m.\u001b[39mminute)\n\u001b[1;32m---> 36\u001b[0m at_night, on_grass \u001b[39m=\u001b[39m at_night_on_grass\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     37\u001b[0m at_night \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mNight Game\u001b[39m\u001b[39m'\u001b[39m \u001b[39m==\u001b[39m at_night\n\u001b[0;32m     38\u001b[0m on_grass \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mon grass\u001b[39m\u001b[39m'\u001b[39m \u001b[39m==\u001b[39m on_grass\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "# year to scrap\n",
    "years = [\"2022\", \"2021\", \"2019\", \"2018\", \"2017\", \"2016\", \"2015\"]\n",
    "y1 = [\"2022\", \"2021\"]\n",
    "y2 = [\"2019\", \"2018\"]\n",
    "y3 = [\"2017\", \"2016\", \"2015\"]\n",
    "\n",
    "scrap_by_years(years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1602"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(f\"gamesData{2022}.pickle\", \"rb\") as output_file:\n",
    "            data = cPickle.load(output_file)\n",
    "len(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "43df744c98c246816ca7c9cbc62e6caeb27475d47e1cd721e7fc30f17cf8524c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
